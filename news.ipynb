{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "news.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1-5sABd7FYsuTMdeEj2vaiK7yvQWUF4kU",
      "authorship_tag": "ABX9TyP/KZfCDp4JVGtFsCCBXfw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elwing-Chou/ml1216/blob/main/news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqdUTIcOGpY_"
      },
      "source": [
        "import pandas as pd\r\n",
        "# pd.options.display.max_rows = 1000\r\n",
        "# pd.options.display.max_colwidth = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3euZXEewAMCq"
      },
      "source": [
        "import zipfile\r\n",
        "f = zipfile.ZipFile(\"drive/MyDrive/news/chinese_news_trans.zip\")\r\n",
        "f.extractall()\r\n",
        "f = zipfile.ZipFile(\"drive/MyDrive/news/chinese_news_test.zip\")\r\n",
        "f.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRzrQR0aB_tY"
      },
      "source": [
        "import os\r\n",
        "import glob\r\n",
        "import pandas as pd\r\n",
        "def load_data(base):\r\n",
        "    datas = {\r\n",
        "        \"content\":[],\r\n",
        "        \"category\":[]\r\n",
        "    }\r\n",
        "    for dn in glob.glob(\"{}/*\".format(base)):\r\n",
        "        small = os.path.join(dn, \"*.txt\")\r\n",
        "        big = os.path.join(dn, \"*.TXT\")\r\n",
        "        for fn in glob.glob(small) + glob.glob(big):\r\n",
        "            with open(fn, encoding=\"utf-8\") as f:\r\n",
        "                news = f.read()\r\n",
        "                datas[\"content\"].append(news)\r\n",
        "                cat = os.path.split(dn)[-1]\r\n",
        "                datas[\"category\"].append(cat)\r\n",
        "    return pd.DataFrame(datas)\r\n",
        "train = load_data(\"chinese_news_trans\")\r\n",
        "test = load_data(\"chinese_news_test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R849-quBEiN0"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvz-W_ThQSMz"
      },
      "source": [
        "# Series: unique/value_counts\r\n",
        "writers = train[\"category\"].unique()\r\n",
        "trans = {n:i for i, n in enumerate(writers)}\r\n",
        "trans_r = {i:n for i, n in enumerate(writers)}\r\n",
        "# Series: replace\r\n",
        "y_train = train[\"category\"].replace(trans)\r\n",
        "y_test = test[\"category\"].replace(trans)\r\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7seZ4xYUP66j"
      },
      "source": [
        "import jieba\r\n",
        "from urllib.request import urlretrieve\r\n",
        "url = \"https://github.com/fxsjy/jieba/raw/master/extra_dict/dict.txt.big\"\r\n",
        "urlretrieve(url, \"dict.txt.big\")\r\n",
        "jieba.set_dictionary(\"dict.txt.big\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18rAnKl3Qjth"
      },
      "source": [
        "def newscut(p):\r\n",
        "    return \" \".join(jieba.cut(p))\r\n",
        "x_train = train[\"content\"].apply(newscut)\r\n",
        "x_test = test[\"content\"].apply(newscut)\r\n",
        "x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tB8ia6dQwIn"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "vec = CountVectorizer()\r\n",
        "x_train_count = vec.fit_transform(x_train)\r\n",
        "x_test_count = vec.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhRG73BTQ6S2"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "clf = MultinomialNB(alpha=0.01)\r\n",
        "clf.fit(x_train_count, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjja_U3ZQ8wm"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "pre = clf.predict(x_test_count)\r\n",
        "accuracy_score(pre, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvBVFcaTRAFv"
      },
      "source": [
        "p = input(\"寫首新聞:\")\r\n",
        "p = [newscut(p)]\r\n",
        "count = vec.transform(p)\r\n",
        "prob = clf.predict_proba(count)[0]\r\n",
        "for w, pr in zip(writers, prob):\r\n",
        "    print(w, round(pr, 4))\r\n",
        "ans = clf.predict(count)[0]\r\n",
        "print(\"最可能是:\", writers[ans])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}